"""
æ–‡æ¡£ç›¸å…³å·¥å…·
"""
import re
from html.parser import HTMLParser
from typing import Optional, Literal, List, Dict, Any
from datetime import datetime
from pydantic import BaseModel, Field
from langchain.tools import BaseTool
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.database import AsyncSessionLocal
from app.models.document import Document
from app.agents.event_manager import AgentEventManager


def parse_html_paragraphs(html_content: str) -> List[Dict[str, Any]]:
    """è§£æHTMLå†…å®¹ï¼Œè¯†åˆ«å—çº§å…ƒç´ ä½œä¸ºæ®µè½"""
    paragraphs = []
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºHTMLæ ¼å¼ï¼ˆåŒ…å«HTMLæ ‡ç­¾ï¼‰
    if not re.search(r'<[^>]+>', html_content):
        # ä¸æ˜¯HTMLæ ¼å¼ï¼ŒæŒ‰åŸæ¥çš„æ–¹å¼å¤„ç†ï¼ˆæŒ‰åŒæ¢è¡Œç¬¦åˆ‡åˆ†ï¼‰
        raw_paragraphs = html_content.split('\n\n')
        current_offset = 0
        
        for idx, para_text in enumerate(raw_paragraphs):
            para_text = para_text.strip()
            if not para_text:
                current_offset += 2
                continue
            
            start_offset = html_content.find(para_text, current_offset)
            if start_offset == -1:
                start_offset = current_offset
            end_offset = start_offset + len(para_text)
            
            paragraphs.append({
                'id': f"p_{idx + 1}",
                'tag': 'text',
                'content': para_text,
                'html_content': para_text,
                'startOffset': start_offset,
                'endOffset': end_offset,
            })
            current_offset = end_offset
        
        return paragraphs
    
    # HTMLæ ¼å¼ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å—çº§å…ƒç´ 
    # åŒ¹é…æ¨¡å¼ï¼š<tag attr="value">content</tag>
    block_elements = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'li', 
                      'blockquote', 'pre', 'article', 'section', 'aside']
    
    # åˆ›å»ºåŒ¹é…æ‰€æœ‰å—çº§å…ƒç´ çš„æ­£åˆ™è¡¨è¾¾å¼
    # åŒ¹é…éè‡ªé—­åˆæ ‡ç­¾ï¼š<tag attr="value">content</tag>
    tag_pattern = '|'.join(block_elements)
    pattern = rf'<({tag_pattern})([^>]*)>(.*?)</\1>'
    
    matches = list(re.finditer(pattern, html_content, re.DOTALL | re.IGNORECASE))
    
    for idx, match in enumerate(matches):
        tag = match.group(1).lower()
        attrs_str = match.group(2)
        inner_html = match.group(3)
        
        # æå–å±æ€§ï¼ˆç‰¹åˆ«æ˜¯idï¼‰
        id_match = re.search(r'id=["\']([^"\']+)["\']', attrs_str)
        element_id = id_match.group(1) if id_match else ""
        
        # æå–çº¯æ–‡æœ¬å†…å®¹ï¼ˆé€’å½’ç§»é™¤å†…éƒ¨HTMLæ ‡ç­¾ï¼‰
        text_content = re.sub(r'<[^>]+>', '', inner_html).strip()
        
        # å¦‚æœæ–‡æœ¬å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡
        if not text_content:
            continue
        
        # è®¡ç®—ä½ç½®
        start_offset = match.start()
        end_offset = match.end()
        
        paragraphs.append({
            'id': element_id if element_id else f"{tag}_{idx + 1}",
            'tag': tag,
            'content': text_content,
            'html_content': match.group(0),  # å®Œæ•´çš„HTMLæ ‡ç­¾åŠå…¶å†…å®¹
            'startOffset': start_offset,
            'endOffset': end_offset,
        })
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•å—çº§å…ƒç´ ï¼Œå°è¯•æŒ‰è¡Œåˆ†å‰²
    if not paragraphs:
        # ä½¿ç”¨HTMLParseræå–æ–‡æœ¬å—
        parser = HTMLTextExtractor()
        parser.feed(html_content)
        text_parts = parser.get_text().split('\n\n')
        
        current_offset = 0
        for idx, text_part in enumerate(text_parts):
            text_part = text_part.strip()
            if not text_part:
                continue
            
            # åœ¨åŸå§‹HTMLä¸­æŸ¥æ‰¾æ–‡æœ¬ä½ç½®ï¼ˆè¿‘ä¼¼ï¼‰
            start_offset = html_content.find(text_part[:20], current_offset)
            if start_offset == -1:
                start_offset = current_offset
            
            paragraphs.append({
                'id': f"p_{idx + 1}",
                'tag': 'text',
                'content': text_part,
                'html_content': text_part,
                'startOffset': start_offset,
                'endOffset': start_offset + len(text_part),
            })
            current_offset = start_offset + len(text_part)
    
    return paragraphs


class HTMLTextExtractor(HTMLParser):
    """ç®€å•çš„HTMLæ–‡æœ¬æå–å™¨ï¼ˆç”¨äºå›é€€æ–¹æ¡ˆï¼‰"""
    def __init__(self):
        super().__init__()
        self.text_parts = []
        self.current_text = ""
        self.block_tags = {'p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 
                          'li', 'blockquote', 'pre', 'br'}
    
    def handle_data(self, data: str):
        self.current_text += data
    
    def handle_starttag(self, tag: str, attrs: list):
        if tag.lower() in self.block_tags:
            if self.current_text.strip():
                self.text_parts.append(self.current_text.strip())
                self.current_text = ""
    
    def handle_endtag(self, tag: str):
        if tag.lower() in {'p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'}:
            if self.current_text.strip():
                self.text_parts.append(self.current_text.strip())
                self.current_text = ""
    
    def get_text(self) -> str:
        if self.current_text.strip():
            self.text_parts.append(self.current_text.strip())
        return "\n\n".join(self.text_parts)


class DocumentReadInput(BaseModel):
    document_id: int


class DocumentReadTool(BaseTool):
    name = "document_reader"
    description = "è¯»å–æŒ‡å®šæ–‡æ¡£å…¨æ–‡å†…å®¹ï¼Œè¾“å…¥ {document_id}."
    args_schema = DocumentReadInput

    def __init__(self, user_id: int):
        super().__init__()
        object.__setattr__(self, 'user_id', user_id)

    async def _arun(self, document_id: int):
        async with AsyncSessionLocal() as session:
            result = await session.execute(
                select(Document).where(Document.id == document_id, Document.author_id == self.user_id)
            )
            document = result.scalar_one_or_none()
            if not document:
                return "Document not found"
            return document.content or ""

    async def _run(self, *args, **kwargs):
        return await self._arun(**kwargs)


# ğŸ†• æ–°å¢: æ–‡æ¡£åˆ†æå·¥å…·ï¼ˆç”¨äºæ®µè½ç¼–è¾‘æ¨¡å¼ï¼‰
class DocumentAnalysisInput(BaseModel):
    """æ–‡æ¡£åˆ†æè¾“å…¥"""
    document_id: int = Field(..., description="éœ€è¦åˆ†æçš„æ–‡æ¡£ID")
    user_intent: str = Field(..., description="ç”¨æˆ·æ„å›¾æè¿°")
    target_selection: Optional[dict] = Field(None, description="ç”¨æˆ·é€‰ä¸­çš„æ–‡æœ¬èŒƒå›´")


class DocumentAnalysisTool(BaseTool):
    """æ–‡æ¡£åˆ†æå·¥å…· - åˆ†ææ–‡æ¡£ç»“æ„å¹¶è¯†åˆ«éœ€è¦ä¿®æ”¹çš„æ®µè½"""
    name = "document_analyzer"
    description = (
        "åˆ†ææ–‡æ¡£ç»“æ„ï¼Œæ ¹æ®ç”¨æˆ·æ„å›¾å’Œé€‰ä¸­æ–‡æœ¬è‡ªåŠ¨è¯†åˆ«éœ€è¦ä¿®æ”¹çš„æ®µè½èŒƒå›´ã€‚"
        "ä½¿ç”¨æ­¤å·¥å…·æ—¶ï¼Œéœ€è¦æä¾›document_idï¼ˆæ–‡æ¡£IDï¼‰å’Œuser_intentï¼ˆç”¨æˆ·æ„å›¾æè¿°ï¼‰ã€‚"
        "å¯ä»¥å¯é€‰æä¾›target_selectionï¼ˆç”¨æˆ·é€‰ä¸­çš„æ–‡æœ¬èŒƒå›´ï¼‰ã€‚"
        "è¿”å›æ–‡æ¡£çš„æ®µè½ç»“æ„åˆ—è¡¨ï¼ŒåŒ…å«æ¯ä¸ªæ®µè½çš„IDã€å†…å®¹å’Œä½ç½®ä¿¡æ¯ã€‚"
    )
    args_schema = DocumentAnalysisInput

    def __init__(self, user_id: int):
        super().__init__()
        object.__setattr__(self, 'user_id', user_id)

    async def _arun(
        self, 
        document_id: int, 
        user_intent: str,
        target_selection: Optional[dict] = None
    ):
        """åˆ†ææ–‡æ¡£å¹¶è¿”å›æ®µè½ç»“æ„"""
        async with AsyncSessionLocal() as session:
            result = await session.execute(
                select(Document).where(
                    Document.id == document_id, 
                    Document.author_id == self.user_id
                )
            )
            document = result.scalar_one_or_none()
            if not document:
                return "Document not found"
            
            content = document.content or ""
            if not content.strip():
                return "Document is empty"
            
            # è§£ææ®µè½ï¼ˆè‡ªåŠ¨è¯†åˆ«HTMLæˆ–çº¯æ–‡æœ¬æ ¼å¼ï¼‰
            raw_paragraphs = parse_html_paragraphs(content)
            paragraphs = []
            
            for idx, para_data in enumerate(raw_paragraphs):
                paragraph_info = {
                    "id": para_data.get('id', f"p_{idx + 1}"),
                    "content": para_data['content'],
                    "type": para_data.get('tag', 'paragraph'),
                    "tag": para_data.get('tag', 'paragraph'),
                    "htmlContent": para_data.get('html_content', para_data['content']),
                    "startOffset": para_data['startOffset'],
                    "endOffset": para_data['endOffset'],
                    "isRelevant": self._is_relevant_to_selection(
                        para_data['startOffset'], para_data['endOffset'], target_selection
                    ),
                    # å¦‚æœæ²¡æœ‰é€‰ä¸­æ–‡æœ¬ï¼Œåˆ™æ‰€æœ‰æ®µè½éƒ½ç›¸å…³ï¼ˆç”¨äºå…¨æ–‡æ¡£æ”¹å†™åœºæ™¯ï¼‰
                    "shouldProcess": target_selection is None or self._is_relevant_to_selection(
                        para_data['startOffset'], para_data['endOffset'], target_selection
                    )
                }
                paragraphs.append(paragraph_info)
            
            # æ„å»ºè¿”å›ç»“æœ
            result_data = {
                "documentId": document_id,
                "totalParagraphs": len(paragraphs),
                "paragraphs": paragraphs,
                "userIntent": user_intent,
                "targetSelection": target_selection
            }
            
            # è¿”å›JSONå­—ç¬¦ä¸²ï¼Œæ–¹ä¾¿æ™ºèƒ½ä½“è§£æ
            import json
            return json.dumps(result_data, ensure_ascii=False, indent=2)

    def _is_relevant_to_selection(
        self, 
        start_offset: int, 
        end_offset: int, 
        target_selection: Optional[dict]
    ) -> bool:
        """åˆ¤æ–­æ®µè½æ˜¯å¦ä¸ç”¨æˆ·é€‰ä¸­çš„æ–‡æœ¬ç›¸å…³"""
        if not target_selection:
            return False
        
        sel_start = target_selection.get("startOffset")
        sel_end = target_selection.get("endOffset")
        
        if sel_start is None or sel_end is None:
            return False
        
        # åˆ¤æ–­æ®µè½ä¸é€‰ä¸­èŒƒå›´æ˜¯å¦æœ‰é‡å 
        return not (end_offset < sel_start or start_offset > sel_end)

    async def _run(self, *args, **kwargs):
        return await self._arun(*args, **kwargs)


# ğŸ†• æ–°å¢: æ®µè½ç¼–è¾‘æŒ‡ä»¤ç”Ÿæˆå·¥å…·
class ParagraphEditInstructionInput(BaseModel):
    """æ®µè½ç¼–è¾‘æŒ‡ä»¤è¾“å…¥"""
    paragraph_id: str = Field(..., description="ç›®æ ‡æ®µè½ID")
    operation: Literal["replace", "delete", "insert_before", "insert_after"] = Field(
        ..., description="æ“ä½œç±»å‹"
    )
    new_content: Optional[str] = Field(None, description="æ–°å†…å®¹ï¼ˆdeleteæ“ä½œæ—¶ä¸ºç©ºï¼‰")
    reasoning: Optional[str] = Field(None, description="ä¿®æ”¹ç†ç”±è¯´æ˜")
    original_content: Optional[str] = Field(None, description="åŸå§‹æ®µè½å†…å®¹")
    start_offset: Optional[int] = Field(None, description="æ®µè½åœ¨æ–‡æ¡£ä¸­çš„èµ·å§‹ä½ç½®")
    end_offset: Optional[int] = Field(None, description="æ®µè½åœ¨æ–‡æ¡£ä¸­çš„ç»“æŸä½ç½®")


class ParagraphEditInstructionTool(BaseTool):
    """æ®µè½ç¼–è¾‘æŒ‡ä»¤ç”Ÿæˆå·¥å…·ï¼ˆä¸ç›´æ¥ä¿®æ”¹æ•°æ®åº“ï¼‰"""
    name = "paragraph_editor"
    description = (
        "ç”Ÿæˆæ®µè½ç¼–è¾‘æŒ‡ä»¤ï¼Œç”¨äºå‰ç«¯å®æ—¶é¢„è§ˆã€‚ä¸ç›´æ¥ä¿®æ”¹æ•°æ®åº“ã€‚"
        "è¿™æ˜¯å‘å‰ç«¯æ¨é€æ–‡æ¡£å†…å®¹çš„å”¯ä¸€æ–¹å¼ï¼Œæ‰€æœ‰ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹éƒ½å¿…é¡»é€šè¿‡æ­¤å·¥å…·æ¨é€ã€‚"
        "è¯·é€ä¸ªæ®µè½è°ƒç”¨æ­¤å·¥å…·ï¼Œæ¯æ¬¡ç”Ÿæˆä¸€ä¸ªæ®µè½çš„ç¼–è¾‘æŒ‡ä»¤ã€‚"
        "ä½¿ç”¨æ­¤å·¥å…·æ—¶ï¼Œå¿…é¡»æä¾›paragraph_idï¼ˆæ®µè½IDï¼‰å’Œoperationï¼ˆæ“ä½œç±»å‹ï¼‰ã€‚"
        "operationå¯é€‰å€¼: replaceï¼ˆæ›¿æ¢æ®µè½ï¼‰ã€deleteï¼ˆåˆ é™¤æ®µè½ï¼‰ã€insert_beforeï¼ˆåœ¨å‰æ’å…¥ï¼‰ã€insert_afterï¼ˆåœ¨åæ’å…¥ï¼‰ã€‚"
        "å¯¹äºæ–°åˆ›å»ºçš„å†…å®¹ï¼ˆæ²¡æœ‰document_idæ—¶ï¼‰ï¼Œä½¿ç”¨insert_afteræ“ä½œï¼Œparagraph_idå¯ä»¥æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„ï¼ˆå¦‚p_1, p_2ç­‰ï¼‰ã€‚"
        "å¯ä»¥å¯é€‰æä¾›new_contentï¼ˆæ–°å†…å®¹ï¼‰ã€reasoningï¼ˆä¿®æ”¹åŸå› ï¼Œå»ºè®®å¡«å†™ï¼‰ã€original_contentï¼ˆåŸå§‹å†…å®¹ï¼Œæ–°å†…å®¹æ—¶å¯ä¸ºç©ºï¼‰ã€start_offsetï¼ˆèµ·å§‹ä½ç½®ï¼‰ã€end_offsetï¼ˆç»“æŸä½ç½®ï¼‰ã€‚"
        "reasoningå­—æ®µç”¨äºå‘ç”¨æˆ·è§£é‡Šä¿®æ”¹åŸå› ï¼Œè¯·åŠ¡å¿…å¡«å†™ã€‚"
        "é‡è¦ï¼šæ‰€æœ‰ç”Ÿæˆçš„æ–‡æ¡£å†…å®¹éƒ½å¿…é¡»é€šè¿‡æ­¤å·¥å…·æ¨é€ï¼Œä¸è¦ç›´æ¥åœ¨æœ€ç»ˆå›å¤ä¸­è¿”å›æ–‡æœ¬å†…å®¹ã€‚"
    )
    args_schema = ParagraphEditInstructionInput

    def __init__(self, event_manager: AgentEventManager, session_id: str, total_paragraphs: int = 0):
        super().__init__()
        object.__setattr__(self, 'event_manager', event_manager)
        object.__setattr__(self, 'session_id', session_id)
        object.__setattr__(self, 'total_paragraphs', total_paragraphs)
        object.__setattr__(self, 'current_progress', 0)

    async def _arun(
        self, 
        paragraph_id: str,
        operation: str,
        new_content: Optional[str] = None,
        reasoning: Optional[str] = None,
        original_content: Optional[str] = None,
        start_offset: Optional[int] = None,
        end_offset: Optional[int] = None
    ):
        """ç”Ÿæˆå¹¶å‘å¸ƒæ®µè½ç¼–è¾‘æŒ‡ä»¤äº‹ä»¶"""
        current_progress = getattr(self, 'current_progress', 0)
        object.__setattr__(self, 'current_progress', current_progress + 1)
        
        # æ„å»ºç¼–è¾‘æŒ‡ä»¤æ•°æ®
        instruction_data = {
            "paragraphId": paragraph_id,
            "operation": operation,
            "newContent": new_content,
            "originalContent": original_content,
            "reasoning": reasoning or f"å¯¹æ®µè½ {paragraph_id} æ‰§è¡Œ {operation} æ“ä½œ",
            "metadata": {
                "startOffset": start_offset,
                "endOffset": end_offset,
                "originalLength": len(original_content) if original_content else 0,
                "newLength": len(new_content) if new_content else 0,
                "confidence": 0.95
            },
            "timestamp": datetime.utcnow().isoformat(),
            "progress": {
                "current": getattr(self, 'current_progress', 0),
                "total": getattr(self, 'total_paragraphs', 0)
            }
        }
        
        # ç«‹å³å‘å¸ƒäº‹ä»¶åˆ°å‰ç«¯
        await self.event_manager.publish(
            self.session_id,
            {
                "type": "paragraph_edit_instruction",
                "data": instruction_data
            }
        )
        
        return f"Edit instruction for paragraph {paragraph_id} generated and sent to frontend"

    async def _run(self, *args, **kwargs):
        return await self._arun(*args, **kwargs)


def create_document_analysis_tool(user_id: int):
    """åˆ›å»ºæ–‡æ¡£åˆ†æå·¥å…·"""
    return DocumentAnalysisTool(user_id=user_id)


def create_paragraph_edit_tool(event_manager: AgentEventManager, session_id: str, total_paragraphs: int = 0):
    """åˆ›å»ºæ®µè½ç¼–è¾‘æŒ‡ä»¤å·¥å…·"""
    return ParagraphEditInstructionTool(
        event_manager=event_manager,
        session_id=session_id,
        total_paragraphs=total_paragraphs
    )
