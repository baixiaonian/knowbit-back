"""
æ›´æ–°ç”¨æˆ·æ–‡æ¡£çš„å‘é‡ç´¢å¼•ï¼ˆç›´æŽ¥è°ƒç”¨æœåŠ¡ï¼Œä¸é€šè¿‡APIï¼‰
"""
import asyncio
import os
from sqlalchemy import select, delete
from app.db.database import AsyncSessionLocal
from app.models.document import Document
from app.models.document_chunk import DocumentChunk
from app.services.vectorizer import DocumentVectorizer


async def update_user_vector_index(user_id: int = 1):
    """æ›´æ–°ç”¨æˆ·æ‰€æœ‰æ–‡æ¡£çš„å‘é‡ç´¢å¼•"""
    
    # 1. èŽ·å–ç”¨æˆ·çš„æ‰€æœ‰æ–‡æ¡£
    async with AsyncSessionLocal() as db:
        result = await db.execute(
            select(Document.id, Document.title, Document.content)
            .where(Document.author_id == user_id)
        )
        docs = result.fetchall()
    
    print("=" * 80)
    print(f"å¼€å§‹æ›´æ–°ç”¨æˆ· {user_id} çš„æ–‡æ¡£å‘é‡ç´¢å¼•")
    print(f"æ€»æ–‡æ¡£æ•°: {len(docs)}")
    print("=" * 80)
    print()
    
    if not docs:
        print("âš ï¸  è¯¥ç”¨æˆ·æ²¡æœ‰æ–‡æ¡£ï¼Œæ— éœ€å‘é‡åŒ–")
        return
    
    # 2. åˆ›å»ºå‘é‡åŒ–æœåŠ¡
    embedding_api_key = os.getenv("EMBEDDING_API_KEY") or "sk-BgRaMMUf3rFV7WszBwp6GjSNSqJLoZhSTILfka4bJwNxLDiw"
    embedding_api_base = os.getenv("EMBEDDING_API_BASE") or "https://aiproxy.bja.sealos.run/v1"
    embedding_model = os.getenv("EMBEDDING_MODEL") or "qwen3-embedding-0.6b"
    
    vectorizer = DocumentVectorizer(
        api_key=embedding_api_key,
        api_base=embedding_api_base,
        model=embedding_model
    )
    
    # 3. æ‰¹é‡å‘é‡åŒ–
    success_count = 0
    fail_count = 0
    total_chunks = 0
    total_tokens = 0
    
    async with AsyncSessionLocal() as db:
        for doc in docs:
            doc_id, doc_title, doc_content = doc
            
            try:
                print(f"ðŸ“„ [{doc_id}] {doc_title}")
                
                # åˆ é™¤æ—§çš„chunks
                await db.execute(
                    delete(DocumentChunk).where(DocumentChunk.document_id == doc_id)
                )
                await db.commit()
                
                # å¤„ç†æ–‡æ¡£ï¼ˆåˆ†å— + å‘é‡åŒ–ï¼‰
                chunks_data = await vectorizer.process_document(
                    content=doc_content or "",
                    metadata={
                        'document_title': doc_title,
                        'author_id': user_id
                    }
                )
                
                if not chunks_data:
                    print(f"  âš ï¸  æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                # ä¿å­˜æ–°çš„chunks
                for chunk_data in chunks_data:
                    chunk = DocumentChunk(
                        document_id=doc_id,
                        content=chunk_data['content'],
                        embedding=chunk_data['embedding'],
                        chunk_index=chunk_data['chunk_index'],
                        token_count=chunk_data['token_count'],
                        chunk_metadata=chunk_data['metadata']
                    )
                    db.add(chunk)
                    total_tokens += chunk_data['token_count']
                
                await db.commit()
                
                print(f"  âœ… æˆåŠŸï¼ç”Ÿæˆ {len(chunks_data)} ä¸ªåˆ†å—ï¼Œå…± {sum(c['token_count'] for c in chunks_data)} tokens")
                success_count += 1
                total_chunks += len(chunks_data)
                
            except Exception as e:
                print(f"  âŒ å¤±è´¥ï¼é”™è¯¯: {str(e)}")
                fail_count += 1
                await db.rollback()
            
            print()
            
            # é¿å…APIé™æµï¼Œç¨ä½œå»¶è¿Ÿ
            await asyncio.sleep(0.5)
    
    # 4. æ˜¾ç¤ºç»Ÿè®¡
    print("=" * 80)
    print("å‘é‡åŒ–å®Œæˆï¼")
    print(f"æˆåŠŸ: {success_count} ä¸ªæ–‡æ¡£")
    print(f"å¤±è´¥: {fail_count} ä¸ªæ–‡æ¡£")
    print(f"æ€»åˆ†å—æ•°: {total_chunks}")
    print(f"æ€»Tokenæ•°: {total_tokens}")
    print("=" * 80)
    
    # 5. æŸ¥çœ‹æœ€ç»ˆç»Ÿè®¡
    async with AsyncSessionLocal() as db:
        result = await db.execute(
            select(DocumentChunk)
            .where(DocumentChunk.document_id.in_([doc[0] for doc in docs]))
        )
        chunks = result.scalars().all()
        
        print()
        print("å‘é‡ç´¢å¼•ç»Ÿè®¡:")
        print(f"  æ€»Chunkæ•°: {len(chunks)}")
        print(f"  å¹³å‡æ¯æ–‡æ¡£: {len(chunks) / success_count if success_count > 0 else 0:.1f} ä¸ªchunks")


if __name__ == "__main__":
    asyncio.run(update_user_vector_index(user_id=1))

